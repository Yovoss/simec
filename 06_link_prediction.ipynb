{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction\n",
    "\n",
    "For two entities _e1_ and _e2_, predict whether the relation _r_ holds between them, e.g., _e1_: Obama, _e2_: USA, _r_: born_in --> True.\n",
    "\n",
    "For the link prediction datasets used here as well as several other models optimized for link prediction, check out https://github.com/ibalazevic/HypER and the associated paper.\n",
    "\n",
    "A SimEc is first trained on all relations combined, then finetuned for individual relations, at which point we use early stopping based on the results on the validation dataset to avoid overfitting.\n",
    "\n",
    "The parameters chosen here are optimized for the WN18 dataset; for other datasets maybe try training for more epochs or use a different learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T12:53:53.185071Z",
     "start_time": "2018-06-14T12:53:51.872586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, division, print_function, absolute_import\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "np.random.seed(28)\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(28)\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "from simec import SimilarityEncoder\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset=\"WN18\", split=\"train\"):\n",
    "    \"\"\"\n",
    "    load the data\n",
    "    \"\"\"\n",
    "    # read in relation data: {relation: {e1: [e2, e4, ...], e2: [e3, e4, ...]}}\n",
    "    # where entities have new ids matching the matrix indices!\n",
    "    # always also save the reverse relation and the collected \"all\" relations that could be used for pretraining\n",
    "    rel_dict = {\"all\": {e: [] for e in sorted_entity_ids}}\n",
    "    df = pd.read_csv(\"data/link_prediction/%s/%s.txt\" % (dataset, split), sep=\"\\t\", names=[\"e1\", \"rel\", \"e2\"])\n",
    "    for i, d in df.iterrows():\n",
    "        if d[\"rel\"] not in rel_dict:\n",
    "            rel_dict[d[\"rel\"]] = {e: [] for e in sorted_entity_ids}\n",
    "            rel_dict[d[\"rel\"]+\"_reverse\"] = {e: [] for e in sorted_entity_ids}\n",
    "        rel_dict[d[\"rel\"]][entity_ids[d[\"e1\"]]].append(entity_ids[d[\"e2\"]])\n",
    "        rel_dict[d[\"rel\"]+\"_reverse\"][entity_ids[d[\"e2\"]]].append(entity_ids[d[\"e1\"]])\n",
    "        rel_dict[\"all\"][entity_ids[d[\"e1\"]]].append(entity_ids[d[\"e2\"]])\n",
    "        rel_dict[\"all\"][entity_ids[d[\"e2\"]]].append(entity_ids[d[\"e1\"]])\n",
    "    return rel_dict\n",
    "\n",
    "\n",
    "def relation_matrix(entity_dict):\n",
    "    \"\"\"\n",
    "    Build a relations matrix that has 1 where two entities have a relation and 0 else\n",
    "\n",
    "    Inputs:\n",
    "        - entity_dict: dict with relations: {e1: [e2, e3, ...]} means entity 1 has relations to 2 and 3\n",
    "                       i.e. something like rel_train[rel] for one relation\n",
    "\n",
    "    Returns:\n",
    "        - n_entities x n_entities relations matrix: for each entity (row), all positions are 1 where this entity\n",
    "                                                    has a relation with other entities\n",
    "    \"\"\"\n",
    "    n = len(sorted_entity_ids)\n",
    "    # transform entity relations into sparse matrix\n",
    "    relmat = dok_matrix((n, n), dtype=np.int8)\n",
    "    for e1 in entity_dict:\n",
    "        relmat[e1, entity_dict[e1]] = 1\n",
    "    relmat = csr_matrix(relmat)\n",
    "    return relmat\n",
    "\n",
    "\n",
    "def eval_relpred(model, target_rels, rel, verbose=0):\n",
    "    \"\"\"\n",
    "    Evaluate the relation prediction\n",
    "\n",
    "    Inputs:\n",
    "        - model: trained model that gives a score for each entity how likely they have some relation\n",
    "        - target_rels: dict with all relations, e.g. rel_val\n",
    "        - rel: which relation we're currently dealing with\n",
    "\n",
    "    Returns:\n",
    "        - hits: list of lists, how often the correct entity ranked at or below 1 - 10\n",
    "        - ranks: actual ranks of target entites\n",
    "    \"\"\"\n",
    "    hits = [[] for i in range(10)]\n",
    "    ranks = []\n",
    "    # target_rels is e.g. rel_test\n",
    "    target_rels = target_rels[rel]\n",
    "    for e1 in target_rels:\n",
    "        if target_rels[e1]:\n",
    "            # predict for the single entity to limit memory consumption\n",
    "            # this gives one vector with scores\n",
    "            pred = model.predict(entity_embeddings[e1])[0]\n",
    "            # for all target objects, save the predicted value\n",
    "            target_pred = {e2: pred[e2] for e2 in target_rels[e1]}\n",
    "            # set all true relations to -1 so they don't mess up the ranking\n",
    "            pred[rel_all[rel][e1]] = -1.\n",
    "            # get ranks for all object entities\n",
    "            for e2 in target_rels[e1]:\n",
    "                pred[e2] = target_pred[e2]\n",
    "                sort_idx = np.argsort(pred)[::-1]\n",
    "                rank = np.where(sort_idx == e2)[0][0]\n",
    "                pred[e2] = -1.\n",
    "                ranks.append(rank+1)\n",
    "                for i in range(10):\n",
    "                    if rank <= i:\n",
    "                        hits[i].append(1.)\n",
    "                    else:\n",
    "                        hits[i].append(0.)\n",
    "    if verbose:\n",
    "        print('Hits @10: {0}'.format(np.mean(hits[9])))\n",
    "        print('Hits @3: {0}'.format(np.mean(hits[2])))\n",
    "        print('Hits @1: {0}'.format(np.mean(hits[0])))\n",
    "        print('Median rank: {0}'.format(np.median(ranks)))\n",
    "        print('Mean rank: {0}'.format(np.mean(ranks)))\n",
    "        print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks))))\n",
    "    return hits, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get a list of all entities\n",
    "dataset = \"WN18\"\n",
    "sorted_entities = set()\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    df = pd.read_csv(\"data/link_prediction/%s/%s.txt\" % (dataset, split), sep=\"\\t\", names=[\"e1\", \"rel\", \"e2\"])\n",
    "    sorted_entities |= set(df[\"e1\"]) | set(df[\"e2\"])\n",
    "sorted_entities = sorted(sorted_entities)\n",
    "sorted_entity_ids = list(range(len(sorted_entities)))\n",
    "# get a mapping to the matrix ids for the entities\n",
    "entity_ids = dict(zip(sorted_entities, sorted_entity_ids))\n",
    "\n",
    "# read in relation data: {relation: {e1: [e2, e4, ...], e2: [e3, e4, ...]}}\n",
    "rel_train = load_data(dataset, split=\"train\")\n",
    "rel_val = load_data(dataset, split=\"valid\")\n",
    "rel_test = load_data(dataset, split=\"test\")\n",
    "rels = sorted(rel_train)\n",
    "# for the FB15K dataset, the validation and test sets are missing some relations\n",
    "for rel in rels:\n",
    "    if rel not in rel_val:\n",
    "        rel_val[rel] = {}\n",
    "    if rel not in rel_test:\n",
    "        rel_test[rel] = {}\n",
    "# for the evaluation we need all true relations between entities\n",
    "rel_all = deepcopy(rel_train)\n",
    "for rel in rel_all:\n",
    "    for e1 in rel_val[rel]:\n",
    "        rel_all[rel][e1].extend(rel_val[rel][e1])\n",
    "    for e1 in rel_test[rel]:\n",
    "        rel_all[rel][e1].extend(rel_test[rel][e1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sparse input matrix that just has ones on the diagonal\n",
    "entity_embeddings = dok_matrix((len(sorted_entity_ids), len(sorted_entity_ids)), dtype=np.int8)\n",
    "for i in sorted_entity_ids:\n",
    "    entity_embeddings[i, i] = 1\n",
    "entity_embeddings = csr_matrix(entity_embeddings)\n",
    "inputs = entity_embeddings\n",
    "# get target matrix for all relations at once\n",
    "rel = \"all\"\n",
    "relmat = relation_matrix(rel_train[rel])\n",
    "\n",
    "# pretraining of the simec to predict all relations\n",
    "e_dim = 100\n",
    "simec = SimilarityEncoder(inputs.shape[1], e_dim, relmat.shape[1], sparse_inputs=True, l2_reg=0., l2_reg_emb=0., l2_reg_out=0.,\n",
    "                          hidden_layers=[(e_dim, 'linear')], ll_activation=\"sigmoid\", loss=\"binary_crossentropy\", opt=0.001)\n",
    "# train the simec for a loooong time and save the weights for later\n",
    "simec.fit(inputs, relmat, epochs=600, batch_size=32, verbose=0)\n",
    "pretrained_weights = deepcopy(simec.model.get_weights())\n",
    "\n",
    "# evaluate: on all relations combined (like it was trained)\n",
    "print(\"#####################\")\n",
    "print(\"rel = all\")\n",
    "print(\"#####################\")\n",
    "print(\"on training data\")\n",
    "hits_rel, ranks_rel = eval_relpred(simec, rel_train, rel, 1)\n",
    "print(\"on validation data\")\n",
    "hits_rel, ranks_rel = eval_relpred(simec, rel_val, rel, 1)\n",
    "print(\"on test data\")\n",
    "hits_rel, ranks_rel = eval_relpred(simec, rel_test, rel, 1)\n",
    "# and on the individual relations\n",
    "hits_train = [[] for i in range(10)]\n",
    "ranks_train = []\n",
    "hits_val = [[] for i in range(10)]\n",
    "ranks_val = []\n",
    "hits_test = [[] for i in range(10)]\n",
    "ranks_test = []\n",
    "for rel in rels:\n",
    "    if rel == \"all\":\n",
    "        continue\n",
    "    hits_rel, ranks_rel = eval_relpred(simec, rel_train, rel, 0)\n",
    "    ranks_train.extend(ranks_rel)\n",
    "    for i in range(10):\n",
    "        hits_train[i].extend(hits_rel[i])\n",
    "    hits_rel, ranks_rel = eval_relpred(simec, rel_val, rel, 0)\n",
    "    ranks_val.extend(ranks_rel)\n",
    "    for i in range(10):\n",
    "        hits_val[i].extend(hits_rel[i])\n",
    "    hits_rel, ranks_rel = eval_relpred(simec, rel_test, rel, 0)\n",
    "    ranks_test.extend(ranks_rel)\n",
    "    for i in range(10):\n",
    "        hits_test[i].extend(hits_rel[i])\n",
    "print(\"#####################\")\n",
    "print(\"averaged results\")\n",
    "print(\"#####################\")\n",
    "print(\"on training data\")\n",
    "print('Hits @10: {0}'.format(np.mean(hits_train[9])))\n",
    "print('Hits @3: {0}'.format(np.mean(hits_train[2])))\n",
    "print('Hits @1: {0}'.format(np.mean(hits_train[0])))\n",
    "print('Median rank: {0}'.format(np.median(ranks_train)))\n",
    "print('Mean rank: {0}'.format(np.mean(ranks_train)))\n",
    "print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks_train))))\n",
    "print(\"on validation data\")\n",
    "print('Hits @10: {0}'.format(np.mean(hits_val[9])))\n",
    "print('Hits @3: {0}'.format(np.mean(hits_val[2])))\n",
    "print('Hits @1: {0}'.format(np.mean(hits_val[0])))\n",
    "print('Median rank: {0}'.format(np.median(ranks_val)))\n",
    "print('Mean rank: {0}'.format(np.mean(ranks_val)))\n",
    "print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks_val))))\n",
    "print(\"on testing data\")\n",
    "print('Hits @10: {0}'.format(np.mean(hits_test[9])))\n",
    "print('Hits @3: {0}'.format(np.mean(hits_test[2])))\n",
    "print('Hits @1: {0}'.format(np.mean(hits_test[0])))\n",
    "print('Median rank: {0}'.format(np.median(ranks_test)))\n",
    "print('Mean rank: {0}'.format(np.mean(ranks_test)))\n",
    "print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simec finetuning for all other rels\n",
    "hits_train = [[] for i in range(10)]\n",
    "ranks_train = []\n",
    "hits_val = [[] for i in range(10)]\n",
    "ranks_val = []\n",
    "hits_test = [[] for i in range(10)]\n",
    "ranks_test = []\n",
    "for rel in rels:\n",
    "    if rel == \"all\" or not rel_val[rel] or not rel_test[rel]:\n",
    "        continue\n",
    "    print(rel)\n",
    "    # get relation matrix e1 -> e2\n",
    "    relmat = relation_matrix(rel_train[rel])\n",
    "    # we only train on the entities for which we actually have relations\n",
    "    e1_idx = sorted(e1 for e1 in rel_train[rel] if rel_train[rel][e1])\n",
    "    relmat = relmat[e1_idx]\n",
    "    inputs = entity_embeddings[e1_idx]\n",
    "    simec = SimilarityEncoder(inputs.shape[1], e_dim, relmat.shape[1], sparse_inputs=True, l2_reg=0., l2_reg_emb=0., l2_reg_out=0.,\n",
    "                              hidden_layers=[(e_dim, 'linear')], ll_activation=\"sigmoid\", loss=\"binary_crossentropy\", opt=0.001)\n",
    "    # set weights with pretrained weights\n",
    "    simec.model.set_weights(pretrained_weights)\n",
    "    # based on the validation data we do early stopping and save the best weights\n",
    "    _, ranks_rel = eval_relpred(simec, rel_val, rel)\n",
    "    best_mrr = np.mean(1./np.array(ranks_rel))\n",
    "    best_epoch = 0\n",
    "    best_weights = deepcopy(simec.model.get_weights())\n",
    "    for i in range(1, 51):\n",
    "        simec.fit(inputs, relmat, epochs=3, batch_size=128, verbose=0)\n",
    "        # evaluate on validation data to avoid overfitting\n",
    "        _, ranks_rel = eval_relpred(simec, rel_val, rel)\n",
    "        mrr = np.mean(1./np.array(ranks_rel))\n",
    "        if mrr > best_mrr:\n",
    "            best_mrr = mrr\n",
    "            best_epoch = i*3\n",
    "            best_weights = deepcopy(simec.model.get_weights())\n",
    "        elif i*3 > 25:\n",
    "            break\n",
    "        print(\"MRR after %i epochs: %.7f (best: %.7f; epoch %i)\" % (i*3, mrr, best_mrr, best_epoch))\n",
    "    simec.model.set_weights(best_weights)\n",
    "    # evaluate\n",
    "    hits_rel, ranks_rel = eval_relpred(simec, rel_train, rel, 0)\n",
    "    ranks_train.extend(ranks_rel)\n",
    "    for i in range(10):\n",
    "        hits_train[i].extend(hits_rel[i])\n",
    "    hits_rel, ranks_rel = eval_relpred(simec, rel_val, rel, 0)\n",
    "    ranks_val.extend(ranks_rel)\n",
    "    for i in range(10):\n",
    "        hits_val[i].extend(hits_rel[i])\n",
    "    hits_rel, ranks_rel = eval_relpred(simec, rel_test, rel, 0)\n",
    "    ranks_test.extend(ranks_rel)\n",
    "    for i in range(10):\n",
    "        hits_test[i].extend(hits_rel[i])\n",
    "    K.clear_session()\n",
    "print(\"#####################\")\n",
    "print(\"averaged results after fine tuning\")\n",
    "print(\"#####################\")\n",
    "print(\"on training data\")\n",
    "print('Hits @10: {0}'.format(np.mean(hits_train[9])))\n",
    "print('Hits @3: {0}'.format(np.mean(hits_train[2])))\n",
    "print('Hits @1: {0}'.format(np.mean(hits_train[0])))\n",
    "print('Median rank: {0}'.format(np.median(ranks_train)))\n",
    "print('Mean rank: {0}'.format(np.mean(ranks_train)))\n",
    "print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks_train))))\n",
    "print(\"on validation data\")\n",
    "print('Hits @10: {0}'.format(np.mean(hits_val[9])))\n",
    "print('Hits @3: {0}'.format(np.mean(hits_val[2])))\n",
    "print('Hits @1: {0}'.format(np.mean(hits_val[0])))\n",
    "print('Median rank: {0}'.format(np.median(ranks_val)))\n",
    "print('Mean rank: {0}'.format(np.mean(ranks_val)))\n",
    "print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks_val))))\n",
    "print(\"on testing data\")\n",
    "print('Hits @10: {0}'.format(np.mean(hits_test[9])))\n",
    "print('Hits @3: {0}'.format(np.mean(hits_test[2])))\n",
    "print('Hits @1: {0}'.format(np.mean(hits_test[0])))\n",
    "print('Median rank: {0}'.format(np.median(ranks_test)))\n",
    "print('Mean rank: {0}'.format(np.mean(ranks_test)))\n",
    "print('Mean reciprocal rank: {0}'.format(np.mean(1./np.array(ranks_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
